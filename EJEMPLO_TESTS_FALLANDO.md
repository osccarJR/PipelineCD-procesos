# ğŸ”´ Ejemplo de Tests Fallando

## ğŸ“Š Resultado de los Tests

```
======================== 3 failed, 58 passed in 0.47s =========================
```

**Resumen:**
- âœ… **58 tests pasaron** correctamente
- âŒ **3 tests fallaron** intencionalmente
- ğŸ“ˆ **95.08% de Ã©xito** (58/61)

---

## âŒ Tests que Fallaron

### 1ï¸âƒ£ **FALLA #1: `test_create_valid_task`**

**UbicaciÃ³n:** `tests/test_task.py:22`

**Error:**
```
AssertionError: assert <TaskPriority.MEDIUM: 2> == <TaskPriority.HIGH: 3>
```

**Â¿QuÃ© pasÃ³?**
- El test esperaba que la prioridad por defecto fuera `HIGH` (Alta)
- Pero el cÃ³digo devuelve `MEDIUM` (Media) como valor por defecto
- **Valor esperado:** `TaskPriority.HIGH`
- **Valor real:** `TaskPriority.MEDIUM`

**CÃ³digo del test:**
```python
def test_create_valid_task(self):
    """Test creating a valid task."""
    task = Task(task_id=1, title="Test Task", description="Test Description")

    # FALLA INTENCIONAL #1: Esperamos prioridad HIGH pero es MEDIUM
    assert task.priority == TaskPriority.HIGH  # âŒ Este test fallÃ³
```

**Â¿CÃ³mo arreglarlo?**
OpciÃ³n A: Cambiar el test para que espere `MEDIUM`:
```python
assert task.priority == TaskPriority.MEDIUM  # âœ…
```

OpciÃ³n B: Cambiar el cÃ³digo para que el default sea `HIGH`:
```python
# En task.py
priority: TaskPriority = TaskPriority.HIGH
```

---

### 2ï¸âƒ£ **FALLA #2: `test_update_title`**

**UbicaciÃ³n:** `tests/test_task.py:126`

**Error:**
```
AssertionError: assert 'Updated Title' == 'Wrong Title'

- Wrong Title
+ Updated Title
```

**Â¿QuÃ© pasÃ³?**
- El test esperaba que el tÃ­tulo fuera `"Wrong Title"`
- Pero el cÃ³digo actualizÃ³ correctamente a `"Updated Title"`
- **Valor esperado:** `"Wrong Title"`
- **Valor real:** `"Updated Title"`

**CÃ³digo del test:**
```python
def test_update_title(self, sample_task):
    """Test updating task title."""
    new_title = "Updated Title"
    sample_task.update_title(new_title)

    # FALLA INTENCIONAL #2: Esperamos un tÃ­tulo diferente
    assert sample_task.title == "Wrong Title"  # âŒ Este test fallÃ³
```

**Â¿CÃ³mo arreglarlo?**
Cambiar el test para que verifique el valor correcto:
```python
assert sample_task.title == "Updated Title"  # âœ…
```

---

### 3ï¸âƒ£ **FALLA #3: `test_get_statistics_with_tasks`**

**UbicaciÃ³n:** `tests/test_task_manager.py:321`

**Error:**
```
assert 50.0 == 75.0
```

**Â¿QuÃ© pasÃ³?**
- El test esperaba una tasa de completitud del **75%**
- El cÃ³digo calculÃ³ correctamente **50%** (2 de 4 tareas completadas)
- **Valor esperado:** `75.0`
- **Valor real:** `50.0`

**Contexto:**
- Total de tareas: 4
- Tareas completadas: 2
- Tasa de completitud: 2/4 = 0.5 = **50%** âœ…

**CÃ³digo del test:**
```python
def test_get_statistics_with_tasks(self, task_manager):
    task1 = task_manager.add_task(title="Task 1")
    task2 = task_manager.add_task(title="Task 2")
    task3 = task_manager.add_task(title="Task 3")
    task_manager.add_task(title="Task 4")

    task_manager.mark_task_completed(task1.task_id)  # Completada
    task_manager.mark_task_completed(task2.task_id)  # Completada
    task_manager.mark_task_in_progress(task3.task_id)

    stats = task_manager.get_statistics()

    # FALLA INTENCIONAL #3: Esperamos 75% pero deberÃ­a ser 50%
    assert stats["completion_rate"] == 75.0  # âŒ Este test fallÃ³
```

**Â¿CÃ³mo arreglarlo?**
Cambiar el test para que verifique el cÃ¡lculo correcto:
```python
assert stats["completion_rate"] == 50.0  # âœ…
```

---

## ğŸ“‹ Salida Completa de pytest

```bash
$ pytest -v

============================= test session starts =============================
platform win32 -- Python 3.11.9, pytest-8.0.0, pluggy-1.5.0

tests/test_task.py::TestTaskCreation::test_create_valid_task FAILED      [  1%]
tests/test_task.py::TestTaskUpdates::test_update_title FAILED            [ 26%]
tests/test_task_manager.py::TestStatistics::test_get_statistics_with_tasks FAILED [ 95%]

================================== FAILURES ===================================

___________________ TestTaskCreation.test_create_valid_task ___________________
tests\test_task.py:22: in test_create_valid_task
    assert task.priority == TaskPriority.HIGH
E   AssertionError: assert <TaskPriority.MEDIUM: 2> == <TaskPriority.HIGH: 3>

______________________ TestTaskUpdates.test_update_title ______________________
tests\test_task.py:126: in test_update_title
    assert sample_task.title == "Wrong Title"
E   AssertionError: assert 'Updated Title' == 'Wrong Title'
E     - Wrong Title
E     + Updated Title

________________ TestStatistics.test_get_statistics_with_tasks ________________
tests\test_task_manager.py:321: in test_get_statistics_with_tasks
    assert stats["completion_rate"] == 75.0
E   assert 50.0 == 75.0

======================== 3 failed, 58 passed in 0.47s =========================
```

---

## ğŸ” Interpretando los Resultados

### SÃ­mbolos de pytest:

- âœ… **PASSED** - Test exitoso
- âŒ **FAILED** - Test fallÃ³
- **[XX%]** - Porcentaje de progreso

### SecciÃ³n de Fallas (`FAILURES`):

Para cada test fallido, pytest muestra:

1. **Nombre del test:** `TestTaskCreation.test_create_valid_task`
2. **UbicaciÃ³n:** `tests\test_task.py:22`
3. **LÃ­nea que fallÃ³:** `assert task.priority == TaskPriority.HIGH`
4. **Error:** `AssertionError: assert <MEDIUM> == <HIGH>`
5. **Contexto adicional:** Valores esperados vs. valores reales

---

## ğŸ› ï¸ Flujo de Trabajo TÃ­pico

### Cuando encuentras tests fallando:

1. **Leer el error**
   ```
   AssertionError: assert 50.0 == 75.0
   ```

2. **Identificar el archivo y lÃ­nea**
   ```
   tests/test_task_manager.py:321
   ```

3. **Analizar quÃ© se esperaba vs. quÃ© se obtuvo**
   ```
   Esperado: 75.0
   Obtenido: 50.0
   ```

4. **Decidir quiÃ©n tiene razÃ³n:**
   - Â¿El test estÃ¡ mal escrito? â†’ Corregir el test
   - Â¿El cÃ³digo tiene un bug? â†’ Corregir el cÃ³digo

5. **Hacer la correcciÃ³n**

6. **Volver a ejecutar los tests**
   ```bash
   pytest
   ```

---

## ğŸ“Š Cobertura de CÃ³digo

Incluso con tests fallando, la cobertura sigue siendo **98.96%**:

```
---------- coverage: platform win32, python 3.11.9-final-0 -----------
Name                             Stmts   Miss Branch BrPart   Cover   Missing
-----------------------------------------------------------------------------
src\task_manager\exceptions.py      12      2      0      0  83.33%   20-21
-----------------------------------------------------------------------------
TOTAL                              152      2     40      0  98.96%

Required test coverage of 90% reached. Total coverage: 98.96%
```

**Â¿Por quÃ©?**
- La **cobertura** mide quÃ© lÃ­neas de cÃ³digo se ejecutan
- Los **tests fallidos** igual ejecutan el cÃ³digo
- Solo que las **aserciones** no coinciden con lo esperado

---

## ğŸ¯ Tipos de Errores Comunes

### 1. **AssertionError**
El valor no coincide con lo esperado
```python
assert result == expected  # âŒ result != expected
```

### 2. **AttributeError**
Intentas acceder a un atributo que no existe
```python
task.nonexistent_field  # âŒ no existe
```

### 3. **TypeError**
Tipo de dato incorrecto
```python
add_task(123)  # âŒ esperaba string
```

### 4. **ValidationError** (personalizado)
Error de validaciÃ³n de datos
```python
Task(task_id=-1, title="")  # âŒ ID negativo o tÃ­tulo vacÃ­o
```

### 5. **TaskNotFoundError** (personalizado)
Tarea no encontrada
```python
manager.get_task(999)  # âŒ no existe
```

---

## ğŸ”§ Comandos Ãštiles

### Ver solo tests fallidos:
```bash
pytest --failed-first
```

### Ver mÃ¡s detalles del error:
```bash
pytest -vv
```

### Ver traceback completo:
```bash
pytest --tb=long
```

### Ejecutar solo un test especÃ­fico:
```bash
pytest tests/test_task.py::TestTaskCreation::test_create_valid_task
```

### Detener en la primera falla:
```bash
pytest -x
```

### Modo detallado + parar en primera falla:
```bash
pytest -xvs
```

---

## ğŸ“ Â¿QuÃ© Aprendimos?

1. âœ… **Los tests detectan errores** antes de que lleguen a producciÃ³n
2. âœ… **pytest muestra claramente** dÃ³nde y por quÃ© fallÃ³
3. âœ… **Los mensajes de error** son descriptivos y Ãºtiles
4. âœ… **Puedes ejecutar tests individualmente** para depurar
5. âœ… **La cobertura se mantiene** incluso con fallas

---

## ğŸ”„ Para Volver a Tests Pasando

### OpciÃ³n 1: Revertir los cambios manualmente

Edita estos 3 archivos y corrige las lÃ­neas:

1. `tests/test_task.py:22`
   ```python
   assert task.priority == TaskPriority.MEDIUM  # Correcto
   ```

2. `tests/test_task.py:126`
   ```python
   assert sample_task.title == "Updated Title"  # Correcto
   ```

3. `tests/test_task_manager.py:321`
   ```python
   assert stats["completion_rate"] == 50.0  # Correcto
   ```

### OpciÃ³n 2: Usar Git (si tienes control de versiones)

```bash
git checkout tests/test_task.py tests/test_task_manager.py
```

---

## ğŸ“ Resumen

| Aspecto | Valor |
|---------|-------|
| Tests totales | 61 |
| Tests pasados | 58 (95.08%) |
| Tests fallados | 3 (4.92%) |
| Cobertura | 98.96% |
| Tiempo de ejecuciÃ³n | 0.47 segundos |

**ConclusiÃ³n:**
Este ejemplo muestra cÃ³mo pytest identifica claramente las fallas, proporciona informaciÃ³n detallada sobre quÃ© saliÃ³ mal, y ayuda a los desarrolladores a corregir los problemas rÃ¡pidamente.

En un proyecto real, **todos los tests deben pasar** antes de hacer commit o deploy a producciÃ³n.

---

## ğŸš€ Siguiente Paso

Para ver todos los tests pasando nuevamente, lee el archivo:
`COMO_ARREGLAR_TESTS.md` (prÃ³ximo archivo a crear)
